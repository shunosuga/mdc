# CPU Training Configuration - Optimized for CPU-only training
# ModernBERT Training Configuration

# Model configuration
model:
  model_name: "answerdotai/ModernBERT-base"
  num_labels: 2
  dropout: 0.1
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

# Data configuration
data:
  train_data_path: "training_data/bert_samples.jsonl"
  validation_split: 0.2
  max_length: 64  # Shorter for CPU training
  batch_size: 4   # Small batch size for CPU
  shuffle_train: true
  shuffle_seed: 42

# Training configuration
training:
  output_dir: "models/modernbert_cpu_test"
  num_epochs: 2   # Fewer epochs for testing
  learning_rate: 3.0e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  gradient_accumulation_steps: 4  # Simulate larger batch size
  max_grad_norm: 1.0
  
  lr_scheduler_type: "linear"
  
  # Early stopping
  early_stopping: true
  early_stopping_patience: 2
  early_stopping_threshold: 0.01

# Hardware configuration
hardware:
  device: "cpu"
  mixed_precision: false  # Disable for CPU
  dataloader_num_workers: 0  # Set to 0 for CPU on Windows
  dataloader_pin_memory: false

# Evaluation configuration
evaluation:
  eval_steps: 50  # More frequent evaluation
  eval_strategy: "steps"
  save_steps: 50
  save_strategy: "steps"
  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  greater_is_better: true

# Logging configuration
logging:
  logging_dir: "logs/modernbert_cpu_training"
  logging_steps: 10  # More frequent logging
  report_to: []
  log_level: "info"
  verbose: true

# Reproducibility
seed: 42

# Advanced training options
advanced:
  gradient_checkpointing: true  # Save memory
  push_to_hub: false
  hub_model_id: null
  class_weights: null
  
  augmentation:
    enabled: false
    dropout_prob: 0.1